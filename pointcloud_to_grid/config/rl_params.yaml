# PPO 설정
ppo:
  total_timesteps: 1000000
  learning_rate: 3.0e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  n_envs: 4

# SAC 설정  
sac:
  total_timesteps: 1000000
  learning_rate: 3.0e-4
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1

# 환경 설정
environment:
  max_episode_steps: 1000
  carla_host: 'localhost'
  carla_port: 2000
  
# 보상 함수 가중치
reward_weights:
  collision_penalty: -100.0
  efficiency_reward: 10.0
  smoothness_reward: 5.0
  parameter_validity_reward: 5.0
  path_length_penalty: -0.5

# Sigmoid 파라미터 범위
sigmoid_bounds:
  M_min: -6.0
  M_max: 6.0
  k_min: 0.2
  k_max: 3.0
  c_min: -10.0
  c_max: 10.0

# 경로 평가 기준
path_evaluation:
  min_path_length: 20
  max_path_length: 50
  optimal_path_length: 35
  smoothness_window: 3
  efficiency_threshold: 0.7
